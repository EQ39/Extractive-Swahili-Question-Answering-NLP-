# Extractive Swahili Question-Answering with DistilBERT focuses on developing a lightweight yet effective natural language processing (NLP) model for answering questions based on Swahili text. Using DistilBERT, a smaller and faster variant of BERT, this approach fine-tunes the model on Kencorpus Swahili Question Answering Dataset to extract precise answers from given passages. The project involves preprocessing Swahili text, handling tokenization challenges, and optimizing the model for accuracy and efficiency. By leveraging transfer learning and Swahili-specific linguistic adaptations, this work aims to improve accessibility to AI-driven information retrieval in Swahili, benefiting education, research, and automated customer support applications. 
